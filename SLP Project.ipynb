{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDXgz6JUxf8",
        "outputId": "a0a1eb0e-70b6-4976-b013-1e73ac077242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=721ebab843398d0e97dbf6597f3ad87b5be1588095116e52eec18556b937f012\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNAwk8bQT5uI",
        "outputId": "8ad8005c-23e0-4005-db5d-1246360f9620"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nSjVgV9UjjY",
        "outputId": "cd8160bd-ec4f-41b9-8dc6-cf061764b7e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcoSf09VU_ah",
        "outputId": "8b6bf6be-c919-4361-dd40-16235fa5b718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/pariza/bbc-news-summary?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8.91M/8.91M [00:00<00:00, 11.5MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset downloaded: /root/.cache/kagglehub/datasets/pariza/bbc-news-summary/versions/2\n"
          ]
        }
      ],
      "source": [
        "#LOAD DATASET (BBC NEWS SUMMARY)\n",
        "\n",
        "path = kagglehub.dataset_download(\"pariza/bbc-news-summary\")\n",
        "print(\"Dataset downloaded:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doi-UAqtbp6Z",
        "outputId": "08f1bce5-9fa3-4103-dbb1-8026ba83c507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully! Total Samples: 2225\n",
            "\n",
            "Samples per category:\n",
            " category\n",
            "sport            511\n",
            "business         510\n",
            "politics         417\n",
            "tech             401\n",
            "entertainment    386\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[1]  Category: entertainment\n",
            " Title: Versace art portfolio up for sale\n",
            "\n",
            " Article:\n",
            "The art collection of murdered fashion designer Gianni Versace could fetch up to £9m ($17m) when it is auctioned in New York and London later this year.\n",
            "\n",
            "Among the pictures for sale are works by Roy Lichtenstein, Andy Warhol and Henri Matisse. The collection was housed at Versace's six-storey New York townhouse. The 51-year-old designer was shot outside his Florida home in 1997 by suspected serial...\n",
            "\n",
            " Summary:\n",
            "Much of the collection will be offered for sale at three auctions in New York in June, with smaller contemporary paintings going under the hammer in London on 22 and 23 June.The collection was housed at Versace's six-storey New York townhouse.The art collection of murdered fashion designer Gianni Versace could fetch up to £9m ($17m) when it is auctioned in New York and London later this year.Among the pictures for sale are works by Roy Lichtenstein, Andy Warhol and Henri Matisse.\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[2]  Category: entertainment\n",
            " Title: Housewives lift Channel 4 ratings\n",
            "\n",
            " Article:\n",
            "The debut of US television hit Desperate Housewives has helped lift Channel 4's January audience share by 12% compared to last year.\n",
            "\n",
            "Other successes such as Celebrity Big Brother and The Simpsons have enabled the broadcaster to surpass BBC Two for the first month since last July. BBC Two's share of the audience fell from 11.2% to 9.6% last month in comparison with January 2004. Celebrity Big Brot...\n",
            "\n",
            " Summary:\n",
            "BBC Two's share of the audience fell from 11.2% to 9.6% last month in comparison with January 2004.The debut of US television hit Desperate Housewives has helped lift Channel 4's January audience share by 12% compared to last year.Its share of the audience soared by 20% in January 2005 compared with last year, and currently stands at an average of 28.6%.The two main television channels, BBC1 and ITV1, have both seen their monthly audience share decline in a year on year comparison for January, while Five's proportion remained the same at a slender 6.3%.\n",
            "------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "articles_dir = os.path.join(path, \"BBC News Summary/News Articles\")\n",
        "summaries_dir = os.path.join(path, \"BBC News Summary/Summaries\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for category in os.listdir(articles_dir):\n",
        "    article_folder = os.path.join(articles_dir, category)\n",
        "    summary_folder = os.path.join(summaries_dir, category)\n",
        "\n",
        "    for filename in os.listdir(article_folder):\n",
        "        article_path = os.path.join(article_folder, filename)\n",
        "        summary_path = os.path.join(summary_folder, filename)\n",
        "\n",
        "        try:\n",
        "            with open(article_path, 'r', encoding='utf-8') as f:\n",
        "                full_text = f.read().strip()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(article_path, 'r', encoding='latin-1') as f:\n",
        "                full_text = f.read().strip()\n",
        "\n",
        "        # Split first line as title and remaining as article\n",
        "        lines = full_text.split(\"\\n\", 1)\n",
        "        title = lines[0].strip()\n",
        "        article = lines[1].strip() if len(lines) > 1 else \"\"\n",
        "\n",
        "        try:\n",
        "            with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "                summary = f.read().strip()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(summary_path, 'r', encoding='latin-1') as f:\n",
        "                summary = f.read().strip()\n",
        "\n",
        "        data.append({\n",
        "            \"category\": category,\n",
        "            \"title\": title,\n",
        "            \"article\": article,\n",
        "            \"summary\": summary\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(f\"Dataset loaded successfully! Total Samples: {len(df)}\")\n",
        "print(\"\\nSamples per category:\\n\", df[\"category\"].value_counts())\n",
        "\n",
        "# --- Display few examples ---\n",
        "for i in range(2):\n",
        "    print(f\"\\n[{i+1}]  Category: {df['category'][i]}\")\n",
        "    print(f\" Title: {df['title'][i]}\")\n",
        "    print(f\"\\n Article:\\n{df['article'][i][:400]}...\")\n",
        "    print(f\"\\n Summary:\\n{df['summary'][i]}\")\n",
        "    print(\"-\"*120)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT9QhM7zVMmN",
        "outputId": "5a7dc0c4-2f0a-478c-e46d-fe8233dbf743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Cleaning dataset...\n",
            "Cleaning done!\n"
          ]
        }
      ],
      "source": [
        "# DATA CLEANING (TEXT PREPROCESSING)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = [w for w in text.split() if w not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "print(\"\\nCleaning dataset...\")\n",
        "df[\"clean_article\"] = df[\"article\"].apply(clean_text)\n",
        "df[\"clean_summary\"] = df[\"summary\"].apply(clean_text)\n",
        "print(\"Cleaning done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2ckpUaPVZ2I",
        "outputId": "8214e6a3-98e0-4c03-f747-cd7a5d98b78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Preview of Cleaned Data:\n",
            "\n",
            "Category: entertainment\n",
            "Original Article (first 200 chars):\n",
            "The art collection of murdered fashion designer Gianni Versace could fetch up to £9m ($17m) when it is auctioned in New York and London later this year.\n",
            "\n",
            "Among the pictures for sale are works by Roy L...\n",
            "\n",
            "Cleaned Article:\n",
            "art collection murdered fashion designer gianni versace could fetch £m auctioned new york london later year among pictures sale works roy lichtenstein andy warhol henri matisse collection housed versa...\n",
            "\n",
            "Original Summary:\n",
            "Much of the collection will be offered for sale at three auctions in New York in June, with smaller contemporary paintings going under the hammer in London on 22 and 23 June.The collection was housed at Versace's six-storey New York townhouse.The art collection of murdered fashion designer Gianni Versace could fetch up to £9m ($17m) when it is auctioned in New York and London later this year.Among the pictures for sale are works by Roy Lichtenstein, Andy Warhol and Henri Matisse.\n",
            "\n",
            "Cleaned Summary:\n",
            "much collection offered sale three auctions new york june smaller contemporary paintings going hammer london junethe collection housed versaces sixstorey new york townhousethe art collection murdered fashion designer gianni versace could fetch £m auctioned new york london later yearamong pictures sale works roy lichtenstein andy warhol henri matisse\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#  Display a few cleaned samples\n",
        "print(\"\\n Preview of Cleaned Data:\\n\")\n",
        "for i in range(1):\n",
        "    print(f\"Category: {df['category'][i]}\")\n",
        "    print(f\"Original Article (first 200 chars):\\n{df['article'][i][:200]}...\\n\")\n",
        "    print(f\"Cleaned Article:\\n{df['clean_article'][i][:200]}...\\n\")\n",
        "    print(f\"Original Summary:\\n{df['summary'][i]}\\n\")\n",
        "    print(f\"Cleaned Summary:\\n{df['clean_summary'][i]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtrE_uBvVuvC",
        "outputId": "b59e301f-e4eb-4fcb-ec48-2943c5cdcbf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 1780, Test size: 445\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWD_reGVcIwY"
      },
      "source": [
        "Extractive Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csi5l_hGV139",
        "outputId": "cfd19667-caa7-46ef-88cf-393e95165459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training TF-IDF model on training data...\n",
            " TF-IDF trained!\n"
          ]
        }
      ],
      "source": [
        "# TRAIN: TF-IDF VECTORIZER + TEXTRANK\n",
        "print(\"\\n Training TF-IDF model on training data...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_vectorizer.fit(train_df[\"clean_article\"])\n",
        "print(\" TF-IDF trained!\")\n",
        "\n",
        "def custom_textrank_summary(text, vectorizer, top_n=3):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if len(sentences) <= top_n:\n",
        "        return text\n",
        "    tfidf_matrix = vectorizer.transform(sentences)\n",
        "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "    n = len(sentences)\n",
        "    scores = np.ones(n)\n",
        "    damping = 0.85\n",
        "    threshold = 0.0001\n",
        "\n",
        "    for _ in range(100):\n",
        "        new_scores = (1 - damping) + damping * sim_matrix.dot(scores) / np.sum(sim_matrix, axis=1)\n",
        "        if np.sum(np.abs(new_scores - scores)) < threshold:\n",
        "            break\n",
        "        scores = new_scores\n",
        "\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]\n",
        "    summary = \" \".join(ranked_sentences[:top_n])\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iCMXiuf3WKzB"
      },
      "outputs": [],
      "source": [
        "train_df = test_df.copy()\n",
        "train_df[\"generated_summary\"] = test_df[\"clean_article\"].apply(\n",
        "    lambda x: custom_textrank_summary(x, tfidf_vectorizer, top_n=3)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkdI1sqfWhxy",
        "outputId": "dbd456a7-7df0-40d1-cc20-3d99e1c9eb7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " TRAIN RESULTS:\n",
            "Avg Cosine Similarity: 0.7817\n",
            "Avg ROUGE-1: 0.5768\n",
            "Avg ROUGE-L: 0.3572\n"
          ]
        }
      ],
      "source": [
        "# EVALUATION\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def cosine_sim_score(t1, t2):\n",
        "    tfidf = tfidf_vectorizer.transform([t1, t2])\n",
        "    return cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
        "\n",
        "train_cos, train_r1, train_rL = [], [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    ref, gen = row[\"clean_summary\"], row[\"generated_summary\"]\n",
        "    train_cos.append(cosine_sim_score(ref, gen))\n",
        "    r = scorer.score(ref, gen)\n",
        "    train_r1.append(r['rouge1'].fmeasure)\n",
        "    train_rL.append(r['rougeL'].fmeasure)\n",
        "\n",
        "print(f\"\\n TRAIN RESULTS:\")\n",
        "print(f\"Avg Cosine Similarity: {np.mean(train_cos):.4f}\")\n",
        "print(f\"Avg ROUGE-1: {np.mean(train_r1):.4f}\")\n",
        "print(f\"Avg ROUGE-L: {np.mean(train_rL):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2h7oze9W9zZ",
        "outputId": "f6985deb-2c7d-4880-d042-4b9adea39013"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " TEST SAMPLE\n",
            "\n",
            "Category: tech\n",
            "\n",
            "Original Article:\n",
            "BT is starting its push into television with plans to offer TV over broadband.\n",
            "\n",
            "As a telecoms company, BT is moving to a content distribution strategy, Andrew Burke, chief of BT's new Entertainment unit told the IPTV World Forum. \"We want to be an entertainment facilitator,\" he said on the opening day of the London conference. The BBC is also trialling a service to play programmes over the net and has not ruled out offering it to non-licence fee payers overseas. The corporation's Interactive Media Player (iMP) is its first foray into broadband TV - known as IPTV (Internet Protocol TV).\n",
            "\n",
            "\"We see several opportunities for delivering the type of content that normally broadcasters find it diffic...\n",
            "\n",
            "--------------------------------------------\n",
            "Human-Written Summary (Dataset):\n",
            "With a broadband net subscription, you can also get your TV and phone service.The BBC recognises that TV over broadband is a reality and aims to innovate with it, said Rahul Chakkara, controller of BBCi's 24/7 interactive TV services.The corporation's Interactive Media Player (iMP) is its first foray into broadband TV - known as IPTV (Internet Protocol TV).It also sees delivering TV over broadband as a way of getting high-definition (HD) content to people sooner than they will be able to get it through conventional, regular broadcasts.According to Benoit Joly from broadband telecoms firm Thales, 30% of Europe cannot get satellite TV or digital TV.Broadband to them would not be about data and the net - that could come later for them - but about cheap phone calls and more choice of TV programmes.BT is starting its push into television with plans to offer TV over broadband.Delivering programmes over broadband offers clear public value, says the BBC, because it gives people more control, and more choice.It is all part of the larger changing TV technology landscape and, like personal digital video recorders (PVRs), gives people much more control over TV.It helps, said Mr Burke, that people are more au fait with terms like \"digital\", \"interactive\", now that digital TV reaches more than 56% of UK homes.But IPTV could be used for home-monitoring, \"pet cams\", localised news services, and local authority TV, too says BT.Analysts say that IPTV will account for 10% of the digital TV market in Europe alone by the end of the decade.BT does not see itself as a broadcaster of IPTV services, rather as an \"enabler\", said Mr Burke.They could get IPTV though.Since it uses internet technology, IPTV could mean more choice of programmes, more, more interactivity, tailored programming, and more localised content outside of conventional satellite, digital cable, and terrestrial broadcasts.Both use broadband net connections to carry information, like video and voice, in packets of data instead of conventional means.\n",
            "\n",
            "--------------------------------------------\n",
            "Generated Summary (TF-IDF + TextRank):\n",
            "bt starting push television plans offer tv broadband telecoms company bt moving content distribution strategy andrew burke chief bts new entertainment unit told iptv world forum want entertainment facilitator said opening day london conference bbc also trialling service play programmes net ruled offering nonlicence fee payers overseas corporations interactive media player imp first foray broadband tv known iptv internet protocol tv see several opportunities delivering type content normally broadcasters find difficult get viewers said bts andrew burke people broadband connection speeds increasing telcos around world looking new ways make money increased competition net service providers encouraged ofcom eroded bts position market looking good return investment technology made broadband adsl reality also sees delivering tv broadband way getting highdefinition hd content people sooner able get conventional regular broadcasts bbcs imp finished successful technical trials set much larger consumer trials later officially launches bbc must show government offers value money delivering programmes broadband offers clear public value says bbc gives people control choice iptv similar idea voip services like skype use broadband net connections carry information like video voice packets data instead conventional means since uses internet technology iptv could mean choice programmes interactivity tailored programming localised content outside conventional satellite digital cable terrestrial broadcasts part larger changing tv technology landscape like personal digital video recorders pvrs gives people much control tv broadcasters see iptv pvrs threat opportunity bbc recognises tv broadband reality aims innovate said rahul chakkara controller bbcis interactive tv services imp based peertopeer technology lets people download programmes bbc owns rights seven days broadcast iptv enables us take back programme audience different times said mr chakkara tell audience programme paid via licence fee access time want helps said mr burke people au fait terms like digital interactive digital tv reaches uk homes according benoit joly broadband telecoms firm thales europe cannot get satellite tv digital tv could get iptv though analysts say iptv account digital tv market europe alone end decade needs happen agree analysts connection speeds bumped handle service mbps connections would ideal bt see broadcaster iptv services rather enabler said mr burke strategy hybrid approach explained overtheair conventional broadcasts supplemented content broadband initially appealing niche markets like sports fans widen iptv could used homemonitoring pet cams localised news services local authority tv says bt even suggests could target households uk computer country broadband would data net could come later cheap phone calls choice tv programmes home choice already offers hours shows channels delivered broadband homes london broadband net subscription also get tv phone service content deals partnerships offers satellite well terrestrial channels bespoke channels based viewers pick choose catalogues aims expand nationally seeing lot success offers subscribers aims double uptake well reach summer although still early stage iptv another application broadband underlines growing prominence backbone network another utility like electricity\n",
            "\n",
            "--------------------------------------------\n",
            "Cosine Similarity for this sample: 0.8442\n"
          ]
        }
      ],
      "source": [
        "# SAMPLE TESTING\n",
        "\n",
        "import random\n",
        "sample_idx = random.randint(0, len(test_df) - 1)\n",
        "sample = test_df.iloc[sample_idx]\n",
        "sample_cosine = cosine_sim_score(sample[\"clean_summary\"], sample[\"generated_summary\"])\n",
        "\n",
        "print(\" TEST SAMPLE\\n\")\n",
        "print(f\"Category: {sample['category']}\")\n",
        "print(f\"\\nOriginal Article:\\n{sample['article'][:700]}...\")\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(f\"Human-Written Summary (Dataset):\\n{sample['summary']}\")\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(f\"Generated Summary (TF-IDF + TextRank):\\n{sample['generated_summary']}\")\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(f\"Cosine Similarity for this sample: {sample_cosine:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIlXdZ2ibECg"
      },
      "source": [
        "Title Generation Using TF-IDF Keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-oDoHo7jZtzP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY-D2dqNbNck",
        "outputId": "bf5d669e-992a-4e45-87cc-7103dc0f4135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Generating Smart Titles using TF-IDF keywords...\n",
            " Titles generated successfully!\n"
          ]
        }
      ],
      "source": [
        "# TITLE GENERATION\n",
        "print(\"\\n Generating Smart Titles using TF-IDF keywords...\")\n",
        "\n",
        "# Reuse the TF-IDF vectorizer trained on article corpus\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "tfidf.fit(train_df[\"clean_article\"])\n",
        "\n",
        "# Extract top keywords for each article\n",
        "def get_keywords(text, vectorizer, top_n=3):\n",
        "    tfidf_matrix = vectorizer.transform([text])\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    scores = tfidf_matrix.toarray()[0]\n",
        "    top_indices = scores.argsort()[-top_n:][::-1]\n",
        "    return [feature_names[i] for i in top_indices]\n",
        "\n",
        "def generate_title(article, category, vectorizer):\n",
        "    keywords = get_keywords(article, vectorizer, top_n=3)\n",
        "    title = f\"{category.capitalize()}: {' '.join(keywords)}\"\n",
        "    return title\n",
        "\n",
        "# Generate titles for test set\n",
        "test_df = test_df.copy()\n",
        "test_df[\"generated_title\"] = test_df.apply(\n",
        "    lambda row: generate_title(row[\"clean_article\"], row[\"category\"], tfidf),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\" Titles generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAFV1SPdbVLY",
        "outputId": "117d0a88-bab6-4a73-f588-17b436ea561e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Evaluating Generated Titles...\n",
            "\n",
            " Avg BLEU Score: 0.0412\n",
            " Avg ROUGE-L Score: 0.1847\n"
          ]
        }
      ],
      "source": [
        "# EVALUATING\n",
        "print(\"\\n Evaluating Generated Titles...\")\n",
        "\n",
        "smooth = SmoothingFunction().method1\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "bleu_scores, rougeL_scores = [], []\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    ref_title = row[\"title\"]\n",
        "    gen_title = row[\"generated_title\"]\n",
        "\n",
        "    ref_tokens = ref_title.lower().split()\n",
        "    gen_tokens = gen_title.lower().split()\n",
        "\n",
        "    bleu = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smooth)\n",
        "    rouge = scorer.score(ref_title.lower(), gen_title.lower())[\"rougeL\"].fmeasure\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    rougeL_scores.append(rouge)\n",
        "\n",
        "print(f\"\\n Avg BLEU Score: {np.mean(bleu_scores):.4f}\")\n",
        "print(f\" Avg ROUGE-L Score: {np.mean(rougeL_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5WizkHqbVAb",
        "outputId": "844573ef-2990-4c3a-c457-c8b167585427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " SAMPLE TITLE GENERATION\n",
            "\n",
            "Category: business\n",
            "\n",
            "Original Title:\n",
            "Jarvis sells Tube stake to Spain\n",
            "\n",
            "Generated Title:\n",
            "Business: jarvis lines stake\n",
            "\n",
            "--------------------------------------------\n",
            "Article Snippet:\n",
            "Shares in engineering group Jarvis have soared more than 16% on news that it is offloading its stake in London underground consortium Tube Lines.\n",
            "\n",
            "The sale of the 33% stake to Spain's Ferrovial for £146m ($281m) is a lifeline to Jarvis, which was weighed down by debts of more than £230m. The company...\n",
            "\n",
            "--------------------------------------------\n",
            "ROUGE-L for this sample: 0.4000\n"
          ]
        }
      ],
      "source": [
        "# SAMPLE TESTING\n",
        "import random\n",
        "idx = random.randint(0, len(test_df)-1)\n",
        "sample = test_df.iloc[idx]\n",
        "\n",
        "print(\"\\n SAMPLE TITLE GENERATION\\n\")\n",
        "print(f\"Category: {sample['category']}\")\n",
        "print(f\"\\nOriginal Title:\\n{sample['title']}\")\n",
        "print(f\"\\nGenerated Title:\\n{sample['generated_title']}\")\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(f\"Article Snippet:\\n{sample['article'][:300]}...\")\n",
        "print(\"\\n--------------------------------------------\")\n",
        "print(f\"ROUGE-L for this sample: {scorer.score(sample['title'], sample['generated_title'])['rougeL'].fmeasure:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtG8tdVxch6k"
      },
      "source": [
        "Title Generation using Tranformer model T5-Small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cT69M_A3ctD-"
      },
      "outputs": [],
      "source": [
        "!pip install transformers rouge-score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XjXBivpctBc",
        "outputId": "49425645-e23b-4d09-8d3e-3bd6d423388e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zHB7cOBwcxfo"
      },
      "outputs": [],
      "source": [
        "df[\"clean_title\"] = df[\"title\"].apply(clean_text)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-9J2cMoRcxcn"
      },
      "outputs": [],
      "source": [
        "# LOAD T5-SMALL MODEL\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Encode input (article) and output (title)\n",
        "def encode_batch(texts, targets, tokenizer, max_input_len=512, max_output_len=32):\n",
        "    inputs = tokenizer(\n",
        "        [\"summarize: \" + text for text in texts],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_input_len\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_output_len\n",
        "    )\n",
        "    return inputs, labels\n",
        "\n",
        "train_inputs, train_labels = encode_batch(\n",
        "    train_df[\"clean_article\"].tolist(),\n",
        "    train_df[\"clean_title\"].tolist(),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "# Move tensors to device\n",
        "train_inputs = {k: v.to(device) for k, v in train_inputs.items()}\n",
        "train_labels = train_labels[\"input_ids\"].to(device)\n",
        "\n",
        "# Training hyperparameters\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
        "epochs = 15\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFzCW3X_cxZ_",
        "outputId": "39d3fe42-6634-4f69-8f64-302392af7ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Training T5 for Title Generation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:07<00:00,  6.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 — Avg Loss: 2.6830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:14<00:00,  6.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/15 — Avg Loss: 1.5171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/15 — Avg Loss: 1.4019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/15 — Avg Loss: 1.3254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/15 — Avg Loss: 1.2754\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/15 — Avg Loss: 1.2229\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/15 — Avg Loss: 1.1739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/15 — Avg Loss: 1.1384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/15 — Avg Loss: 1.0975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 — Avg Loss: 1.0591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 — Avg Loss: 1.0185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 — Avg Loss: 0.9846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/15 — Avg Loss: 0.9551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14/15 — Avg Loss: 0.9198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:10<00:00,  6.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15/15 — Avg Loss: 0.9056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "print(\"\\n Training T5 for Title Generation...\")\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = []\n",
        "    for i in tqdm(range(0, len(train_labels), batch_size)):\n",
        "        input_batch = {k: v[i:i+batch_size] for k, v in train_inputs.items()}\n",
        "        label_batch = train_labels[i:i+batch_size]\n",
        "        outputs = model(**input_batch, labels=label_batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        epoch_loss.append(loss.item())\n",
        "    print(f\"Epoch {epoch+1}/{epochs} — Avg Loss: {np.mean(epoch_loss):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVIqDaYkdY7B",
        "outputId": "a737fc54-2538-4ccc-c2fd-4b8068f468b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 445/445 [01:05<00:00,  6.82it/s]\n"
          ]
        }
      ],
      "source": [
        "# EVALUATION\n",
        "model.eval()\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "bleu_scores, rougeL_scores = [], []\n",
        "\n",
        "for i in tqdm(range(len(test_df))):\n",
        "    article = test_df.iloc[i][\"clean_article\"]\n",
        "    ref_title = test_df.iloc[i][\"clean_title\"]\n",
        "\n",
        "    # Generate title\n",
        "    inputs = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "    gen_ids = model.generate(**inputs, max_length=32, num_beams=4)\n",
        "    gen_title = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # BLEU\n",
        "    bleu = sentence_bleu([ref_title.split()], gen_title.split(), smoothing_function=smooth)\n",
        "    # ROUGE-L\n",
        "    rouge = scorer.score(ref_title, gen_title)[\"rougeL\"].fmeasure\n",
        "\n",
        "    bleu_scores.append(bleu)\n",
        "    rougeL_scores.append(rouge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li3pjC_Ldiz9",
        "outputId": "5b79f297-9b52-49d9-82a3-342b8a030f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Title Generation Results\n",
            " Avg BLEU Score: 0.0842\n",
            " Avg ROUGE-L Score: 0.3416\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Title Generation Results\")\n",
        "print(f\" Avg BLEU Score: {np.mean(bleu_scores):.4f}\")\n",
        "print(f\" Avg ROUGE-L Score: {np.mean(rougeL_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbygbylbmAWu",
        "outputId": "951b8481-9858-4cfe-b1b2-73a5d3414c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " SAMPLE TITLE GENERATION\n",
            "Category: sport\n",
            "\n",
            "Original Title:\n",
            "Dementieva prevails in Hong Kong\n",
            "\n",
            "Generated Title:\n",
            "dementieva wins hong kongs title\n",
            "\n",
            "Article Snippet:\n",
            " Elena Dementieva swept aside defending champion Venus Williams 6-3 6-2 to win Hong Kong's Champions Challenge event.\n",
            "\n",
            "The Russian, ranked sixth in the world, broke Williams three times in the first set, while losing her service once. Williams saved three championship points before losing the match a ...\n",
            "\n",
            "ROUGE-L for this sample: 0.6667\n"
          ]
        }
      ],
      "source": [
        "# TESTING SAMPLE\n",
        "import random\n",
        "idx = random.randint(0, len(test_df)-1)\n",
        "sample = test_df.iloc[idx]\n",
        "\n",
        "inputs = tokenizer(\"summarize: \" + sample[\"clean_article\"], return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "gen_ids = model.generate(**inputs, max_length=32, num_beams=4)\n",
        "generated_title = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n SAMPLE TITLE GENERATION\")\n",
        "print(f\"Category: {sample['category']}\")\n",
        "print(f\"\\nOriginal Title:\\n{sample['title']}\")\n",
        "print(f\"\\nGenerated Title:\\n{generated_title}\")\n",
        "print(\"\\nArticle Snippet:\\n\", sample[\"article\"][:300], \"...\")\n",
        "print(f\"\\nROUGE-L for this sample: {scorer.score(sample['clean_title'], generated_title)['rougeL'].fmeasure:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPqHwv_K4jFy"
      },
      "source": [
        "Complete Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWdM-utB6dwp",
        "outputId": "4bafa7a2-65d5-4aa3-c44e-62510e1826f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ORIGINAL ARTICLE:\n",
            " Serena Williams claimed her seventh Wimbledon title and 22nd Grand Slam overall after defeating Angelique Kerber in straight sets.\n",
            "With this win, Williams equals Steffi Graf’s record and cements her status as one of the greatest tennis players of all time.\n",
            "\n",
            "ORIGINAL TITLE:\n",
            " Serena Williams Wins Seventh Wimbledon Title\n",
            "\n",
            "GENERATED SUMMARY:\n",
            " serena williams claimed seventh wimbledon title nd grand slam overall defeating angelique kerber straight sets win williams equals steffi graf’s record cements status one greatest tennis players time\n",
            "\n",
            "GENERATED TITLE:\n",
            " williams wins wimbledon title\n",
            "\n",
            "EVALUATION RESULTS:\n",
            "BLEU Score        : 0.1457\n",
            "ROUGE-L (F1)      : 0.8000\n",
            "Cosine Similarity : 0.7093\n"
          ]
        }
      ],
      "source": [
        "# Example input (you can replace this with any article)\n",
        "original_title = \"Serena Williams Wins Seventh Wimbledon Title\"\n",
        "custom_article = \"\"\"\n",
        "Serena Williams claimed her seventh Wimbledon title and 22nd Grand Slam overall after defeating Angelique Kerber in straight sets.\n",
        "With this win, Williams equals Steffi Graf’s record and cements her status as one of the greatest tennis players of all time.\n",
        "\"\"\"\n",
        "\n",
        "# ---- Clean the input ----\n",
        "clean_custom_article = clean_text(custom_article)\n",
        "\n",
        "# ---- Generate Summary using TF-IDF + TextRank ----\n",
        "generated_summary = custom_textrank_summary(clean_custom_article, tfidf_vectorizer, top_n=3)\n",
        "\n",
        "# ---- Generate Title using Fine-tuned T5 ----\n",
        "inputs = tokenizer(\n",
        "    \"summarize: \" + clean_custom_article,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512\n",
        ").to(device)\n",
        "\n",
        "gen_ids = model.generate(**inputs, max_length=32, num_beams=6)\n",
        "generated_title = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ---- Compute Evaluation Metrics ----\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# BLEU\n",
        "bleu_score = sentence_bleu(\n",
        "    [original_title.lower().split()],\n",
        "    generated_title.lower().split(),\n",
        "    smoothing_function=SmoothingFunction().method1\n",
        ")\n",
        "\n",
        "# ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(original_title.lower(), generated_title.lower())\n",
        "rouge_L = scores['rougeL'].fmeasure\n",
        "\n",
        "# Cosine Similarity\n",
        "vectorizer = TfidfVectorizer().fit([original_title.lower(), generated_title.lower()])\n",
        "vectors = vectorizer.transform([original_title.lower(), generated_title.lower()])\n",
        "cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "\n",
        "# ---- Display Results ----\n",
        "print(\"\\nORIGINAL ARTICLE:\\n\", custom_article.strip())\n",
        "print(\"\\nORIGINAL TITLE:\\n\", original_title.strip())\n",
        "print(\"\\nGENERATED SUMMARY:\\n\", generated_summary.strip())\n",
        "print(\"\\nGENERATED TITLE:\\n\", generated_title.strip())\n",
        "\n",
        "print(\"\\nEVALUATION RESULTS:\")\n",
        "print(f\"BLEU Score        : {bleu_score:.4f}\")\n",
        "print(f\"ROUGE-L (F1)      : {rouge_L:.4f}\")\n",
        "print(f\"Cosine Similarity : {cosine_sim:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
